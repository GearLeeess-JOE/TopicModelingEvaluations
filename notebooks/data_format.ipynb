{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "\n",
    "import pandas as pd\n",
    "DATA_PATH = \"/home/kw215/Documents/research_codes/Topic-modeling-evaluations/notebooks/mod_actions.csv\"\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text.\n",
    "\n",
    "    Parameters:\n",
    "    - text: Text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - Preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    # Remove URLs\n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    # # Remove all non-ASCII characters\n",
    "    # text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    # # Remove all non-word characters (everything except numbers and letters)\n",
    "    # import re\n",
    "    # text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    # # remove all digits\n",
    "    # text = re.sub(r\"\\d+\", '', text)\n",
    "    # # remove all single characters\n",
    "    # text = re.sub(r\"\\b[a-zA-Z]\\b\", '', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the given path and preprocesses it.\n",
    "\n",
    "    Parameters:\n",
    "    - path: Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing the processed data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[(df['action'] == 'removelink') | (df['action'] == 'approvelink')]\n",
    "    df = df[(df['mod'] != 'AutoModerator') & (df['target_title'] != '[ Removed by Reddit ]')]\n",
    "    # df['text'] = 'Question: ' + df['target_title'] + '\\n' + 'Description: ' + df['target_body']\n",
    "    df['text'] = df['target_title'] + '\\n' + df['target_body']\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['text'],keep='last')\n",
    "    print(df['text'].head())\n",
    "    return df\n",
    "\n",
    "def output_results(topic_model, sub_topic_models):\n",
    "    \"\"\"\n",
    "    Outputs the results of the topic modeling.\n",
    "\n",
    "    Parameters:\n",
    "    - topic_model: The main topic model.\n",
    "    - sub_topic_models: Dictionary of sub-topic models.\n",
    "\n",
    "    This function should be extended to format and output the results as required.\n",
    "    \"\"\"\n",
    "    # Example output code (to be modified as per specific requirements)\n",
    "    print(\"Main Topics:\")\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    for topic_id, model in sub_topic_models.items():\n",
    "        print(f\"Sub-Topics for Topic {topic_id}:\")\n",
    "        print(model.get_topic_info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     good benadryl dosage for beginners\\nwhat’s a g...\n",
      "2     psilocybin mushrooms and possible false memory...\n",
      "11    am i shadowbanned or hacked  no i’m not on dru...\n",
      "12    in a hypothetical experiment if someone was gi...\n",
      "13    mg retarded fucking brother may be in danger w...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_topic_modeling(dataset):\n",
    "    # Flatten the dataset to find the unique words and sort them\n",
    "    unique_words = sorted(set(word for doc in dataset for word in doc))\n",
    "\n",
    "    # Write the sorted vocabulary to 'vocabulary.txt'\n",
    "    with open('vocabulary.txt', 'w') as vocab_file:\n",
    "        for word in unique_words:\n",
    "            vocab_file.write(f\"{word}\\n\")\n",
    "\n",
    "    # Write the corpus to 'corpus.tsv', using actual words\n",
    "    with open('corpus.tsv', 'w') as corpus_file:\n",
    "        for doc in dataset:\n",
    "            # Join the words in the document with a space and write to file\n",
    "            corpus_file.write(' '.join(doc) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [v.split() for v in data['text'].values]\n",
    "format_data_for_topic_modeling(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmeva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
